{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN and LRP on Social Physical brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how saliency maps are computed for social and physical pain brain dataset. We will first train a model and then apply LRP methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import h5py\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "\n",
    "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "mnistutils = imp.load_source(\"utils_mnist\", \"../utils_mnist.py\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Load the dataset and keep some images from the test set for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(batch_size, tr_data):\n",
    "   \n",
    "    while True:\n",
    "        \"\"\"\n",
    "        #this is working everytime model trained - making Batch-size data\n",
    "        \"\"\"\n",
    "        tr_sample_count = tr_data['X_data'].shape[0]\n",
    "        tr_sample_idxs = range(0,tr_sample_count)\n",
    "\n",
    "        # Batch_count \n",
    "\n",
    "        batch_count = tr_sample_count / batch_size\n",
    "        if tr_sample_count % batch_size:\n",
    "            batch_count = batch_count+1\n",
    "\n",
    "        # Yield X,y(batch_size), every for loop \n",
    "        for i in range(0, int(batch_count)):\n",
    "            if i == batch_count-1:\n",
    "                batch_size_idxs = tr_sample_idxs[i*batch_size : ]\n",
    "            else : \n",
    "                batch_size_idxs = tr_sample_idxs[i*batch_size : (i+1)*batch_size]\n",
    "\n",
    "            batch_size_idxs = sorted(batch_size_idxs)\n",
    "            X = tr_data['X_data'][batch_size_idxs].reshape(len(batch_size_idxs),68,95,79)\n",
    "            y = tr_data['y_data'][batch_size_idxs]\n",
    "\n",
    "            yield X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 32\n",
    "data_file_name = '180404_social_physical_masked_cross.hdf5'\n",
    "label_to_class_name = [str(i) for i in range(2)]\n",
    "selection_index_LRP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The next part trains and evaluates a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_model_cnn_2D():\n",
    "       \n",
    "\n",
    "    model = Sequential() #0\n",
    "    model.add(Conv2D(8, (3,3), kernel_initializer='he_normal', padding='same', input_shape=(68,95,79)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(16, (3,3), kernel_initializer='he_normal', padding='same'))#4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', padding='same'))#7\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', padding='same')) #10\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Flatten()) #13\n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(2, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "#     model.add(Dense(1, activation='softmax', kernel_initializer='he_normal'))\n",
    "    \n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create & train model\n",
    "def train_model():\n",
    "\n",
    "    \"\"\"\n",
    "    train model with data in every cross_i. Everytime model trained, get_result and generate_sequences are called\n",
    "    \"\"\"\n",
    "    # Option : when you don't have enough GPUs\n",
    "#     with tf.device('/cpu:0'):\n",
    "\n",
    "    with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "        for i in range(0,3):  # i is the cross number 59\n",
    "\n",
    "            print('this is ith iter : ' , i)\n",
    "\n",
    "            tr_data_X_name = 'cross_'+str(i+1)+ '_X'+'_train'\n",
    "            tr_data_y_name = 'cross_'+str(i+1)+ '_y'+'_train'\n",
    "            te_data_X_name = 'cross_'+str(i+1)+ '_X'+'_test'\n",
    "            te_data_y_name = 'cross_'+str(i+1)+ '_y'+'_test'\n",
    "\n",
    "            tr_data = {}\n",
    "            tr_data['X_data'] = np.array(data[tr_data_X_name])\n",
    "            tr_data['y_data'] = np.array(data[tr_data_y_name])\n",
    "            \n",
    "            te_data = {}\n",
    "            te_data['X_data'] = np.array(data[te_data_X_name])\n",
    "            te_data['y_data'] = np.array(data[te_data_y_name])\n",
    "        \n",
    "      \n",
    "\n",
    "            \n",
    "            tr_data['y_data'] = keras.utils.to_categorical(tr_data['y_data'], 2)\n",
    "            te_data['y_data'] = keras.utils.to_categorical(te_data['y_data'], 2)\n",
    "   \n",
    "            training_sample_count = tr_data['X_data'].shape[0]\n",
    "\n",
    "\n",
    "            # Initialize model\n",
    "\n",
    "            model = make_custom_model_cnn_2D()\n",
    "            model.compile(loss=\"categorical_crossentropy\",\n",
    "                          optimizer=Adam(),\n",
    "                          metrics=[\"accuracy\"])\n",
    "            history = model.fit(tr_data['X_data'],tr_data['y_data'],\n",
    "                                batch_size=mini_batch_size,\n",
    "                                epochs=20,\n",
    "                                verbose=1,shuffle=True)\n",
    "            score = model.evaluate(te_data['X_data'], te_data['y_data'], verbose=0)\n",
    "            return model,score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "Epoch 1/20\n",
      "64/64 [==============================] - 1s 17ms/step - loss: 1.5238 - acc: 0.4375\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7527 - acc: 0.6250\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7641 - acc: 0.5312\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.5956 - acc: 0.5625\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.4485 - acc: 0.8906\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.4789 - acc: 0.7188\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.4154 - acc: 0.8594\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.3277 - acc: 0.9844\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.3083 - acc: 0.9844\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.2508 - acc: 0.9844\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.2061 - acc: 0.9844\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1742 - acc: 0.9844\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1312 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1039 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0808 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0560 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0439 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0221 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0175 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model,score = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "0.75\n",
      "this is ith iter :  1\n",
      "1.0\n",
      "this is ith iter :  2\n",
      "1.0\n",
      "mean of validation accuracy 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_acc = []\n",
    "with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "    for i in range(0,3):  # i is the cross number 59\n",
    "\n",
    "        print('this is ith iter : ' , i)\n",
    "\n",
    "        te_data_X_name = 'cross_'+str(i+1)+ '_X'+'_test'\n",
    "        te_data_y_name = 'cross_'+str(i+1)+ '_y'+'_test'\n",
    "\n",
    "        te_data = {}\n",
    "        te_data['X_data'] = np.array(data[te_data_X_name])\n",
    "        te_data['y_data'] = np.array(data[te_data_y_name])\n",
    "        \n",
    "        te_data['y_data'] = keras.utils.to_categorical(te_data['y_data'], 2)\n",
    "        \n",
    "\n",
    "        score = model.evaluate(te_data['X_data'], te_data['y_data'], verbose=0)\n",
    "        print(score[1])\n",
    "        val_acc.append(score[1])\n",
    "\n",
    "print(\"mean of validation accuracy\",np.mean(val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up a list of analysis methods by preparing tuples containing the methods' string identifiers used by `innvestigate.analyzer.create_analyzer(...)`, some optional parameters, a post processing choice for visualizing the computed analysis and a title for the figure to render. Analyzers can be deactivated by simply commenting the corresponding lines, or added by creating a new tuple as below.\n",
    "\n",
    "For a full list of methods refer to the dictionary `investigate.analyzer.analyzers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure analysis methods and properties\n",
    "if selection_index_LRP == None:\n",
    "    methods = [\n",
    "    # NAME                    OPT.PARAMS                POSTPROC FXN               TITLE\n",
    "\n",
    "    # Show input\n",
    "    (\"lrp.epsilon\",           {\"epsilon\": 1,\"neuron_selection_mode\":\"max_activation\"},           ivis.heatmap,        \"LRP-Epsilon\"),\n",
    "    ]\n",
    "else:    \n",
    "    methods = [\n",
    "        # NAME                    OPT.PARAMS                POSTPROC FXN               TITLE\n",
    "\n",
    "        # Show input\n",
    "        (\"lrp.epsilon\",           {\"epsilon\": 1, \"neuron_selection_mode\" : \"index\"},           ivis.heatmap,        \"LRP-Epsilon\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop below will now instantiate the analyzer objects based on the loaded/trained model and the analyzers' parameterizations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model without trailing softmax\n",
    "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
    "\n",
    "# Create analyzers.\n",
    "analyzers = []\n",
    "for method in methods:\n",
    "    \n",
    "    analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
    "                                            model_wo_softmax, # model without softmax output\n",
    "                                            **method[1])      # optional analysis parameters\n",
    "\n",
    "    # Some analyzers require training.\n",
    "\n",
    "    analyzer.fit(data, batch_size=256, verbose=1)\n",
    "    analyzers.append(analyzer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we analyze each image with the different analyzers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "[('ground truth label : 0', 'pre-softmax logits : 1.95', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 4.14', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 2.37', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 2.42', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : -1.53', 'probabilistic softmax output   : 0.73', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 1.61', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 0.72', 'probabilistic softmax output   : 0.81', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 1.43', 'probabilistic softmax output   : 0.91', 'predicted label : 0'), ('ground truth label : 1', 'pre-softmax logits : 3.51', 'probabilistic softmax output   : 1.00', 'predicted label : 1'), ('ground truth label : 1', 'pre-softmax logits : 2.60', 'probabilistic softmax output   : 0.99', 'predicted label : 1')]\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "\n",
    "with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "    for i in range(0,1):  # i is the cross number 59\n",
    "\n",
    "        print('this is ith iter : ' , i)\n",
    "\n",
    "        tr_data_X_name = 'cross_'+str(i+1)+ '_X'+'_test'\n",
    "        tr_data_y_name = 'cross_'+str(i+1)+ '_y'+'_test'\n",
    "\n",
    "        tr_data = {}\n",
    "        tr_data['X_data'] = np.array(data[tr_data_X_name])\n",
    "        tr_data['y_data'] = np.array(data[tr_data_y_name])\n",
    "        \n",
    "        test_data = tr_data['X_data']\n",
    "        test_label = tr_data['y_data']\n",
    "\n",
    "        n = 10\n",
    "        test_images = list(zip(test_data[:n], test_label[:n]))\n",
    "\n",
    "        analysis = np.zeros([len(test_images), len(analyzers), 68, 95, 3])\n",
    "        text = []\n",
    "        R=[]\n",
    "\n",
    "\n",
    "\n",
    "        for p, (x, y) in enumerate(test_images):\n",
    "            # Add batch axis.\n",
    "            x = x[None, :, :, :]\n",
    "\n",
    "            # Predict final activations, probabilites, and label.\n",
    "            presm = model_wo_softmax.predict_on_batch(x)[0]\n",
    "            prob = model.predict_on_batch(x)[0]\n",
    "            y_hat = prob.argmax()\n",
    "\n",
    "\n",
    "            # Save prediction info:\n",
    "            text.append((\"ground truth label : %s\" % label_to_class_name[int(y)],    # ground truth label\n",
    "                         \"pre-softmax logits : %.2f\" % presm.max(),             # pre-softmax logits\n",
    "                         \"probabilistic softmax output   : %.2f\" % prob.max(),              # probabilistic softmax output  \n",
    "                         \"predicted label : %s\" % label_to_class_name[y_hat] # predicted label\n",
    "                        ))\n",
    "\n",
    "            for aidx, analyzer in enumerate(analyzers):\n",
    "                # Analyze.\n",
    "                if selection_index_LRP == None:\n",
    "                    a = analyzer.analyze(x)\n",
    "                else:\n",
    "                    a = analyzer.analyze(x, neuron_selection=selection_index_LRP)\n",
    "                R.append(a) # Save natual R\n",
    "                \n",
    "                \"\"\"\n",
    "                # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
    "                a = mnistutils.postprocess(a)\n",
    "                # Apply analysis postprocessing, e.g., creating a heatmap.\n",
    "                a = methods[aidx][2](a)\n",
    "                # Store the analysis.\n",
    "                analysis[i, aidx] = a[0]\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "        # Save Relevance as\n",
    "        R_name = 'social_physical_brain_relevance_cross_'+str(i)+'.npy'\n",
    "        np.save(R_name, R)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
