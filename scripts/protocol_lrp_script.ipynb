{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN and LRP on Social Physical brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how saliency maps are computed for social and physical pain brain dataset. We will first train a model and then apply LRP methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import h5py\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "\n",
    "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable\n",
    "Set the variable for making model and the analysis.\n",
    "\n",
    "selection_index_LRP : if you want to see the LRP results with the 1 for Social score and 0 for Physical score and None for the most model prediction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 128\n",
    "data_file_name = '../data/lrp_data/mini_example_social_physical_masked_cross.hdf5'\n",
    "label_to_class_name = [str(i) for i in range(2)]\n",
    "selection_index_LRP = 0 # 0, 1, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The next part trains and evaluates a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make CNN model with 6 layers, Flatten() is inserted between CNN and fully connected layers.\n",
    "def make_custom_model_cnn_2D():\n",
    "       \n",
    "\n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(8, (3,3), kernel_initializer='he_normal', padding='same', input_shape=(68,95,79)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(16, (3,3), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(2, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for creating & training model\n",
    "def train_model():\n",
    "\n",
    "    \"\"\"\n",
    "    train model with data in every cross_i. Everytime model trained, get_result and generate_sequences are called\n",
    "    \"\"\"\n",
    "    # Option : when you don't have enough GPUs\n",
    "#     with tf.device('/cpu:0'):\n",
    "\n",
    "    with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "        for i in range(0,3):  # i is the cross number 59\n",
    "\n",
    "            print('this is ith iter : ' , i)\n",
    "\n",
    "            tr_data_X_name = 'cross_'+str(i+1)+ '_X'+'_train'\n",
    "            tr_data_y_name = 'cross_'+str(i+1)+ '_y'+'_train'\n",
    "            te_data_X_name = 'cross_'+str(i+1)+ '_X'+'_test'\n",
    "            te_data_y_name = 'cross_'+str(i+1)+ '_y'+'_test'\n",
    "\n",
    "            tr_data = {}\n",
    "            tr_data['X_data'] = np.array(data[tr_data_X_name])\n",
    "            tr_data['y_data'] = np.array(data[tr_data_y_name])\n",
    "            \n",
    "            te_data = {}\n",
    "            te_data['X_data'] = np.array(data[te_data_X_name])\n",
    "            te_data['y_data'] = np.array(data[te_data_y_name])\n",
    "            \n",
    "            tr_data['y_data'] = keras.utils.to_categorical(tr_data['y_data'], 2)\n",
    "            te_data['y_data'] = keras.utils.to_categorical(te_data['y_data'], 2)\n",
    "   \n",
    "            training_sample_count = tr_data['X_data'].shape[0]\n",
    "\n",
    "\n",
    "            # Initialize and compile the model\n",
    "            model = make_custom_model_cnn_2D()\n",
    "            model.compile(loss=\"categorical_crossentropy\",\n",
    "                          optimizer=Adam(),\n",
    "                          metrics=[\"accuracy\"])\n",
    "            history = model.fit(tr_data['X_data'],tr_data['y_data'],\n",
    "                                batch_size=mini_batch_size,\n",
    "                                epochs=20,\n",
    "                                verbose=1)\n",
    "            score = model.evaluate(te_data['X_data'], te_data['y_data'], verbose=0)\n",
    "            return model,score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "Epoch 1/20\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 1.4811 - acc: 0.4688\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7014 - acc: 0.6406\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.7998 - acc: 0.5312\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.5704 - acc: 0.7188\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.4441 - acc: 0.7344\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.4814 - acc: 0.6250\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.3890 - acc: 0.8438\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.3105 - acc: 0.9531\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.2911 - acc: 0.9375\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.2488 - acc: 0.9688\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1907 - acc: 0.9844\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1665 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1359 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0999 - acc: 0.9844\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0831 - acc: 0.9844\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0670 - acc: 0.9844\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0470 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0334 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0270 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0199 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Create & train model\n",
    "model,score = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "0.8125\n",
      "this is ith iter :  1\n",
      "1.0\n",
      "this is ith iter :  2\n",
      "1.0\n",
      "mean of validation accuracy 0.9375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_acc = [] #List for save accuracy\n",
    "with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "    for i in range(0,3):  # i is the cross number 59\n",
    "\n",
    "        print('this is ith iter : ' , i)\n",
    "\n",
    "        te_data_X_name = 'cross_'+str(i+1)+ '_X'+'_test'\n",
    "        te_data_y_name = 'cross_'+str(i+1)+ '_y'+'_test'\n",
    "\n",
    "        te_data = {}\n",
    "        te_data['X_data'] = np.array(data[te_data_X_name])\n",
    "        te_data['y_data'] = np.array(data[te_data_y_name])\n",
    "        \n",
    "        te_data['y_data'] = keras.utils.to_categorical(te_data['y_data'], 2)\n",
    "        \n",
    "\n",
    "        score = model.evaluate(te_data['X_data'], te_data['y_data'], verbose=0)\n",
    "        print(score[1])\n",
    "        val_acc.append(score[1])\n",
    "\n",
    "print(\"mean of validation accuracy\",np.mean(val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up a list of analysis methods used by `innvestigate.analyzer.create_analyzer(...)`, some optional parameters, a post processing choice for visualizing the computed analysis and a title for the figure to render. \n",
    "For a full list of methods refer to the dictionary `investigate.analyzer.analyzers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure analysis methods and properties\n",
    "if selection_index_LRP == None:\n",
    "    methods = [\n",
    "    # NAME                    OPT.PARAMS                POSTPROC FXN               TITLE\n",
    "\n",
    "    # Show input\n",
    "    (\"lrp.epsilon\",           {\"epsilon\": 1,\"neuron_selection_mode\":\"max_activation\"},           ivis.heatmap,        \"LRP-Epsilon\"),\n",
    "    ]\n",
    "else:    \n",
    "    methods = [\n",
    "        # NAME                    OPT.PARAMS                POSTPROC FXN               TITLE\n",
    "\n",
    "        # Show input\n",
    "        (\"lrp.epsilon\",           {\"epsilon\": 1, \"neuron_selection_mode\" : \"index\"},           ivis.heatmap,        \"LRP-Epsilon\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop below will now instantiate the analyzer objects based on the loaded/trained model and the analyzers' parameterizations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model without trailing softmax\n",
    "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
    "\n",
    "# Create analyzers.\n",
    "analyzers = []\n",
    "for method in methods:\n",
    "    \n",
    "    analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
    "                                            model_wo_softmax, # model without softmax output\n",
    "                                            **method[1])      # optional analysis parameters\n",
    "\n",
    "    # Some analyzers require training.\n",
    "\n",
    "    analyzer.fit(data, batch_size=256, verbose=1)\n",
    "    analyzers.append(analyzer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we analyze each image with the different analyzers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "[('ground truth label : 0', 'pre-softmax logits : 3.68', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 6.79', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 3.61', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 3.40', 'probabilistic softmax output   : 1.00', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 2.13', 'probabilistic softmax output   : 0.75', 'predicted label : 1'), ('ground truth label : 0', 'pre-softmax logits : 3.48', 'probabilistic softmax output   : 0.99', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 2.33', 'probabilistic softmax output   : 0.96', 'predicted label : 0'), ('ground truth label : 0', 'pre-softmax logits : 2.51', 'probabilistic softmax output   : 0.97', 'predicted label : 0'), ('ground truth label : 1', 'pre-softmax logits : 3.20', 'probabilistic softmax output   : 0.99', 'predicted label : 1'), ('ground truth label : 1', 'pre-softmax logits : 2.88', 'probabilistic softmax output   : 0.98', 'predicted label : 1')]\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "\n",
    "with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "    for i in range(0,1):  # i is the cross number 59\n",
    "\n",
    "        print('this is ith iter : ' , i)\n",
    "\n",
    "        tr_data_X_name = 'cross_'+str(i+1)+ '_X'+'_test'\n",
    "        tr_data_y_name = 'cross_'+str(i+1)+ '_y'+'_test'\n",
    "\n",
    "        tr_data = {}\n",
    "        tr_data['X_data'] = np.array(data[tr_data_X_name])\n",
    "        tr_data['y_data'] = np.array(data[tr_data_y_name])\n",
    "        \n",
    "        test_data = tr_data['X_data']\n",
    "        test_label = tr_data['y_data']\n",
    "\n",
    "        n = 10\n",
    "        test_images = list(zip(test_data[:n], test_label[:n]))\n",
    "\n",
    "        analysis = np.zeros([len(test_images), len(analyzers), 68, 95, 3])\n",
    "        text = []\n",
    "        R=[]\n",
    "\n",
    "\n",
    "\n",
    "        for p, (x, y) in enumerate(test_images):\n",
    "            # Add batch axis.\n",
    "            x = x[None, :, :, :]\n",
    "\n",
    "            # Predict final activations, probabilites, and label.\n",
    "            presm = model_wo_softmax.predict_on_batch(x)[0]\n",
    "            prob = model.predict_on_batch(x)[0]\n",
    "            y_hat = prob.argmax()\n",
    "\n",
    "\n",
    "            # Save prediction info:\n",
    "            text.append((\"ground truth label : %s\" % label_to_class_name[int(y)],    # ground truth label\n",
    "                         \"pre-softmax logits : %.2f\" % presm.max(),             # pre-softmax logits\n",
    "                         \"probabilistic softmax output   : %.2f\" % prob.max(),              # probabilistic softmax output  \n",
    "                         \"predicted label : %s\" % label_to_class_name[y_hat] # predicted label\n",
    "                        ))\n",
    "\n",
    "            for aidx, analyzer in enumerate(analyzers):\n",
    "                # Analyze.\n",
    "                if selection_index_LRP == None:\n",
    "                    a = analyzer.analyze(x)\n",
    "                else:\n",
    "                    a = analyzer.analyze(x, neuron_selection=selection_index_LRP)\n",
    "                R.append(a) # Save natual R\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        # Save Relevance as\n",
    "        R_name = 'social_physical_brain_relevance_cross_'+str(i)+'.npy'\n",
    "        np.save(R_name, R)\n",
    "        print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
