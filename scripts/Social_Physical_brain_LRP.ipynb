{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare analyzers on Social Physical brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how saliency maps are computed for social and physical pain brain dataset. We will first train a model and then apply LRP methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import h5py\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "\n",
    "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "mnistutils = imp.load_source(\"utils_mnist\", \"../utils_mnist.py\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Load the dataset and keep some images from the test set for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sequences(batch_size, tr_data):\n",
    "   \n",
    "    while True:\n",
    "        \"\"\"\n",
    "        #this is working everytime model trained - making Batch-size data\n",
    "        \"\"\"\n",
    "        tr_sample_count = tr_data['X_data'].shape[0]\n",
    "        tr_sample_idxs = range(0,tr_sample_count)\n",
    "\n",
    "        # Batch_count \n",
    "\n",
    "        batch_count = tr_sample_count / batch_size\n",
    "        if tr_sample_count % batch_size:\n",
    "            batch_count = batch_count+1\n",
    "\n",
    "        # Yield X,y(batch_size), every for loop \n",
    "        for i in range(0, int(batch_count)):\n",
    "            if i == batch_count-1:\n",
    "                batch_size_idxs = tr_sample_idxs[i*batch_size : ]\n",
    "            else : \n",
    "                batch_size_idxs = tr_sample_idxs[i*batch_size : (i+1)*batch_size]\n",
    "\n",
    "            batch_size_idxs = sorted(batch_size_idxs)\n",
    "            X = tr_data['X_data'][batch_size_idxs].reshape(len(batch_size_idxs),68,95,79)\n",
    "            y = tr_data['y_data'][batch_size_idxs]\n",
    "\n",
    "            yield X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 32\n",
    "data_file_name = '180404_social_physical_masked_cross.hdf5'\n",
    "label_to_class_name = [str(i) for i in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The next part trains and evaluates a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_custom_model_cnn_2D():\n",
    "       \n",
    "\n",
    "    model = Sequential() #0\n",
    "    model.add(Conv2D(8, (3,3), kernel_initializer='he_normal', padding='same', input_shape=(68,95,79)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(16, (3,3), kernel_initializer='he_normal', padding='same'))#4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', padding='same'))#7\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', padding='same')) #10\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Flatten()) #13\n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='softmax', kernel_initializer='he_normal'))\n",
    "    \n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create & train model\n",
    "def train_model():\n",
    "\n",
    "    \"\"\"\n",
    "    train model with data in every cross_i. Everytime model trained, get_result and generate_sequences are called\n",
    "    \"\"\"\n",
    "    # Option : when you don't have enough GPUs\n",
    "#     with tf.device('/cpu:0'):\n",
    "\n",
    "    with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "        for i in range(0,1):  # i is the cross number 59\n",
    "\n",
    "            print('this is ith iter : ' , i)\n",
    "\n",
    "            tr_data_X_name = 'cross_'+str(i+1)+ '_X'+'_train'\n",
    "            tr_data_y_name = 'cross_'+str(i+1)+ '_y'+'_train'\n",
    "\n",
    "            tr_data = {}\n",
    "            tr_data['X_data'] = np.array(data[tr_data_X_name])\n",
    "            tr_data['y_data'] = np.array(data[tr_data_y_name])\n",
    "\n",
    "            training_sample_count = tr_data['X_data'].shape[0]\n",
    "\n",
    "            # Initialize model\n",
    "\n",
    "            model = make_custom_model_cnn_2D()\n",
    "\n",
    "            # Fit_generator \n",
    "\n",
    "            training_sequence_generator = generate_sequences(mini_batch_size,tr_data)\n",
    "\n",
    "\n",
    "            # Train model with fit_generator\n",
    "\n",
    "            model.fit_generator(generator=training_sequence_generator,steps_per_epoch=(training_sample_count/(mini_batch_size)+1),epochs=10,verbose=1)    ### step_per_epoch is the number of batches\n",
    "                                                          ### generator= is the function that makes batches\n",
    "                                                          ### workers= is the number of multiprocessing  workers=3\n",
    "            return model        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 699ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7.9712 - acc: 0.5000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 7.9712 - acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# model = load_model('2018-02-14_model_saved_LRPh5-Copy1.h5')\n",
    "# # json_file = open(\"model.json\", \"r\") \n",
    "# # loaded_model_json = json_file.read() \n",
    "# # json_file.close() \n",
    "# # loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# # loaded_model.load_weights(\"model.h5\") \n",
    "# # print(\"Loaded model from disk\")\n",
    "\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up a list of analysis methods by preparing tuples containing the methods' string identifiers used by `innvestigate.analyzer.create_analyzer(...)`, some optional parameters, a post processing choice for visualizing the computed analysis and a title for the figure to render. Analyzers can be deactivated by simply commenting the corresponding lines, or added by creating a new tuple as below.\n",
    "\n",
    "For a full list of methods refer to the dictionary `investigate.analyzer.analyzers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure analysis methods and properties\n",
    "methods = [\n",
    "    # NAME                    OPT.PARAMS                POSTPROC FXN               TITLE\n",
    "\n",
    "    # Show input\n",
    "    (\"lrp.epsilon\",           {\"epsilon\": 1},           ivis.heatmap,        \"LRP-Epsilon\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ith iter :  0\n",
      "(16, 68, 95, 79)\n"
     ]
    }
   ],
   "source": [
    " with h5py.File(data_file_name, \"r\") as data:\n",
    "\n",
    "    for i in range(0,1):  # i is the cross number 59\n",
    "\n",
    "        print('this is ith iter : ' , i)\n",
    "\n",
    "        tr_data_X_name = 'cross_'+str(i+1)+ '_X'+'_test'\n",
    "        tr_data_y_name = 'cross_'+str(i+1)+ '_y'+'_test'\n",
    "\n",
    "        tr_data = {}\n",
    "        tr_data['X_data'] = np.array(data[tr_data_X_name])\n",
    "        tr_data['y_data'] = np.array(data[tr_data_y_name])\n",
    "        \n",
    "test_data = tr_data['X_data']\n",
    "test_label = tr_data['y_data']\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop below will now instantiate the analyzer objects based on the loaded/trained model and the analyzers' parameterizations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model without trailing softmax\n",
    "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
    "\n",
    "# Create analyzers.\n",
    "analyzers = []\n",
    "for method in methods:\n",
    "    analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
    "                                            model_wo_softmax, # model without softmax output\n",
    "                                            **method[1])      # optional analysis parameters\n",
    "\n",
    "    # Some analyzers require training.\n",
    "    analyzer.fit(data, batch_size=256, verbose=1)\n",
    "    analyzers.append(analyzer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we analyze each image with the different analyzers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "n = 10\n",
    "test_images = list(zip(test_data[:n], test_label[:n]))\n",
    "\n",
    "analysis = np.zeros([len(test_images), len(analyzers), 68, 95, 3])\n",
    "text = []\n",
    "R=[]\n",
    "\n",
    "\n",
    "\n",
    "for i, (x, y) in enumerate(test_images):\n",
    "    # Add batch axis.\n",
    "    print(x.shape)\n",
    "    x = x[None, :, :, :]\n",
    "    \n",
    "    # Predict final activations, probabilites, and label.\n",
    "    presm = model_wo_softmax.predict_on_batch(x)[0]\n",
    "    prob = model.predict_on_batch(x)[0]\n",
    "    y_hat = prob.argmax()\n",
    "\n",
    "    \n",
    "    # Save prediction info:\n",
    "    text.append((\"%s\" % label_to_class_name[int(y)],    # ground truth label\n",
    "                 \"%.2f\" % presm.max(),             # pre-softmax logits\n",
    "                 \"%.2f\" % prob.max(),              # probabilistic softmax output  \n",
    "                 \"%s\" % label_to_class_name[y_hat] # predicted label\n",
    "                ))\n",
    "\n",
    "    for aidx, analyzer in enumerate(analyzers):\n",
    "        # Analyze.\n",
    "        a = analyzer.analyze(x)\n",
    "        R.append(a)\n",
    "        # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
    "        a = mnistutils.postprocess(a)\n",
    "        # Apply analysis postprocessing, e.g., creating a heatmap.\n",
    "        a = methods[aidx][2](a)\n",
    "        # Store the analysis.\n",
    "        analysis[i, aidx] = a[0]\n",
    "    print(len(R))\n",
    "    np.save('social_physical_brain_relevance.npy', R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(analysis.shape)\n",
    "# # Prepare the grid as rectengular list\n",
    "# grid = [[analysis[i, j] for j in range(analysis.shape[1])]\n",
    "#         for i in range(analysis.shape[0])]\n",
    "# # Prepare the labels\n",
    "# label, presm, prob, pred = zip(*text)\n",
    "# row_labels_left = [('label: {}'.format(label[i]), 'pred: {}'.format(pred[i])) for i in range(len(label))]\n",
    "# row_labels_right = [('logit: {}'.format(presm[i]), 'prob: {}'.format(prob[i])) for i in range(len(label))]\n",
    "# col_labels = [''.join(method[3]) for method in methods]\n",
    "\n",
    "# # Plot the analysis.\n",
    "# eutils.plot_image_grid(grid, row_labels_left, row_labels_right, col_labels,\n",
    "#                        file_name=os.environ.get(\"PLOTFILENAME\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
